# Whisper Segment-Based Transcriber

This project provides a simple and efficient script for transcribing long audio files using **OpenAI Whisper** by splitting the audio into fixed-length segments and writing the transcription to a `.txt` file **as soon as each segment is processed**.

The script is designed for users who want real-time transcription output while handling long-duration audio files such as lectures, meetings, interviews, podcasts, and recordings.


## ğŸš€ Features

- **Segment-based transcription**  
  Splits the audio into equal-length segments (default: 30 seconds).

- **Real-time TXT writing**  
  Each segment is transcribed and appended to a text file immediately.

- **Automatic timestamp labels**  
  Outputs lines such as:  
```

[  0.0â€“ 30.0s] Transcribed text...

````

- **Uses Whisper's official `load_audio()`**  
Ensures consistent loading and resampling to 16 kHz.

- **CPU-friendly**  
`fp16=False` is set by default to ensure compatibility without requiring GPU.

---

## ğŸ“¦ Requirements

### FFmpeg  
The script depends on FFmpeg for decoding audio formats.

Check installation:
```bash
ffmpeg -version
````

### Python packages

Install dependencies using:

```bash
pip install openai-whisper torch
```

Or put them in `requirements.txt`:

```
openai-whisper
torch
```


## ğŸ“ Example Python Script

This repository includes the following transcription script:

```python
import os
import math
import whisper
from whisper.audio import load_audio

# ========= Settings =========
wav_path = "files.wav"  # Input audio file
segment_sec = 30        # Split audio every N seconds

# ========= Load Whisper model =========
print("ğŸ”„ Loading Whisper model: small ...")
model = whisper.load_model("small")
print("âœ… Model loaded\n")

# ========= Load audio using Whisper's loader =========
print(f"ğŸ§ Reading audio file: {wav_path}")
audio = load_audio(wav_path)   # Returns 16 kHz float32 1D array
sr = 16000                     # Whisper uses 16k sample rate

total_samples = len(audio)
audio_duration = total_samples / sr
print(f"ğŸ§ Total audio duration: {audio_duration:.1f} sec")

segment_samples = int(segment_sec * sr)
total_segments = math.ceil(total_samples / segment_samples)
print(f"ğŸ”ª Will be split into {total_segments} segments ({segment_sec} sec each)\n")

# ========= Prepare output file =========
out_path = os.path.splitext(wav_path)[0] + ".txt"
if os.path.exists(out_path):
    os.remove(out_path)

# ========= Segment transcription with streaming write =========
with open(out_path, "a", encoding="utf-8") as f:
    for i in range(total_segments):
        start = i * segment_samples
        end = min((i + 1) * segment_samples, total_samples)
        segment_audio = audio[start:end]

        # Skip extremely short tail segments (< 0.5 sec)
        if len(segment_audio) < sr * 0.5:
            continue

        start_time = start / sr
        end_time = end / sr

        print(f"â³ Processing segment {i+1}/{total_segments}, time {start_time:.1f}â€“{end_time:.1f} sec")

        result = model.transcribe(
            segment_audio,
            fp16=False,      # CPU mode forces fp16 off
            language="zh",   # Set fixed language (Chinese)
            verbose=False
        )
        text = (result.get("text") or "").strip()

        print(f"ğŸ“£ Segment {i+1} text: {text}\n")

        if text:
            f.write(f"[{start_time:6.1f}â€“{end_time:6.1f}s] {text}\n")

print("ğŸ‰ Finished processing")
print(f"ğŸ“„ Output text file: {out_path}")
```

---

## â–¶ï¸ How to Use

Place your audio file as:

```
files.wav
```

Then run:

```bash
python transcribe_segments.py
```

(Or whatever the script filename is.)

A text file with the same base name will be generated:

```
files.txt
```

Example output:

```
[   0.0â€“  30.0s] Hello, this is the beginning of the audio...
[  30.0â€“  60.0s] Here the speaker continues talking...
```

## ğŸ§  Notes

* Whisper automatically converts audio to 16 kHz mono.
* Larger models (e.g., medium, large) improve accuracy but require more memory.
* For GPU acceleration, change:

  ```python
  fp16=True
  ```

  and ensure CUDA is installed.
* Extremely short tail fragments are skipped to prevent blank output.

## ğŸ“œ License

MIT License. Free to modify and use.

## â­ Acknowledgments

This project is powered by **OpenAI Whisper**
[https://github.com/openai/whisper](https://github.com/openai/whisper)



# Whisper Segment-Based Transcriberï¼ˆä¸­æ–‡èªªæ˜ï¼‰

é€™å€‹å°ˆæ¡ˆæä¾›ä¸€å€‹ç°¡å–®é«˜æ•ˆçš„è…³æœ¬ï¼Œå¯ä½¿ç”¨ **OpenAI Whisper** å°é•·éŸ³æª”é€²è¡ŒèªéŸ³è¾¨è­˜ã€‚
é€éå°‡éŸ³è¨Šæª”åˆ‡æˆå›ºå®šç§’æ•¸çš„æ®µè½ï¼Œæ¯è™•ç†å®Œä¸€æ®µå°±ç«‹åˆ»å°‡è¾¨è­˜çµæœå¯«å…¥ `.txt` æª”ï¼Œ**ä¸éœ€è¦ç­‰å¾…æ•´å€‹éŸ³æª”è™•ç†å®Œç•¢**ï¼Œéå¸¸é©åˆè™•ç†é•·æ™‚é–“éŒ„éŸ³ï¼Œä¾‹å¦‚èª²å ‚éŒ„éŸ³ã€æœƒè­°ã€è¨ªè«‡ã€Podcastã€ä¸€èˆ¬èªéŸ³ç´€éŒ„ç­‰ã€‚

---

## ğŸš€ åŠŸèƒ½ç‰¹è‰²

* **åˆ†æ®µè¾¨è­˜**
  å°‡éŸ³è¨Šä¾å›ºå®šé•·åº¦åˆ‡å‰²ï¼ˆé è¨­ï¼š30 ç§’ï¼‰ã€‚

* **å³æ™‚å¯«å…¥ TXT**
  æ¯æ®µè¾¨è­˜å®Œæˆå¾Œï¼Œå³æ™‚è¿½åŠ å¯«å…¥è¼¸å‡ºæª”æ¡ˆã€‚

* **è‡ªå‹•æ™‚é–“æ¨™ç±¤**
  ä¾‹å¦‚ï¼š

```
[  0.0â€“ 30.0s] é€™æ˜¯è¾¨è­˜å…§å®¹...
```

* **ä½¿ç”¨ Whisper å®˜æ–¹ `load_audio()`**
  ç¢ºä¿éŸ³è¨Šä»¥ä¸€è‡´æ–¹å¼è¼‰å…¥ä¸¦é‡æ¡æ¨£è‡³ 16 kHzã€‚

* **CPU å‹å–„æ¨¡å¼**
  é è¨­ `fp16=False`ï¼Œé¿å…å› ç¼ºå°‘ GPU é€ æˆéŒ¯èª¤ã€‚

## ğŸ“¦ ç³»çµ±éœ€æ±‚

### FFmpeg

è…³æœ¬ä¾è³´ FFmpeg è§£ç¢¼éŸ³è¨Šæ ¼å¼ï¼Œè«‹å…ˆå®‰è£ï¼š

æª¢æŸ¥æ˜¯å¦å®‰è£ï¼š

```bash
ffmpeg -version
```

### Python å¥—ä»¶

å®‰è£å¿…è¦å¥—ä»¶ï¼š

```bash
pip install openai-whisper torch
```

æˆ–æ”¾å…¥ `requirements.txt`ï¼š

```
openai-whisper
torch
```

## ğŸ“ Python è…³æœ¬ç¯„ä¾‹

æ­¤å°ˆæ¡ˆåŒ…å«ä»¥ä¸‹èªéŸ³è¾¨è­˜è…³æœ¬ï¼š

```python
import os
import math
import whisper
from whisper.audio import load_audio

# ========= è¨­å®š =========
wav_path = "files.wav"  # ä½ çš„éŸ³æª”
segment_sec = 30        # æ¯æ®µå¹¾ç§’åˆ‡ä¸€æ®µ

# ========= è¼‰å…¥ Whisper æ¨¡å‹ =========
print("ğŸ”„ è¼‰å…¥ Whisper æ¨¡å‹ small ...")
model = whisper.load_model("small")
print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ\n")

# ========= ç”¨ Whisper å®˜æ–¹çš„ load_audio è®€æª” =========
print(f"ğŸ§ æ­£åœ¨è®€å–éŸ³è¨Šæª”ï¼š{wav_path}")
audio = load_audio(wav_path)   # å›å‚³ 16kHzã€float32ã€ä¸€ç¶­ array
sr = 16000                     # Whisper å›ºå®šç”¨ 16k

total_samples = len(audio)
audio_duration = total_samples / sr
print(f"ğŸ§ éŸ³è¨Šç¸½é•·åº¦ï¼šç´„ {audio_duration:.1f} ç§’")

segment_samples = int(segment_sec * sr)
total_segments = math.ceil(total_samples / segment_samples)
print(f"ğŸ”ª å°‡åˆ‡æˆ {total_segments} æ®µï¼ˆæ¯æ®µ {segment_sec} ç§’ï¼‰\n")

# ========= æº–å‚™è¼¸å‡ºæª” =========
out_path = os.path.splitext(wav_path)[0] + ".txt"
if os.path.exists(out_path):
    os.remove(out_path)

# ========= åˆ†æ®µè½‰æ–‡å­— + å³æ™‚å¯«å…¥ =========
with open(out_path, "a", encoding="utf-8") as f:
    for i in range(total_segments):
        start = i * segment_samples
        end = min((i + 1) * segment_samples, total_samples)
        segment_audio = audio[start:end]

        # å¦‚æœé€™æ®µå¤ªçŸ­ï¼ˆä¾‹å¦‚æœ€å¾Œå‰©ä¸åˆ° 0.5 ç§’ï¼‰ï¼Œå°±ç›´æ¥è·³é
        if len(segment_audio) < sr * 0.5:
            continue

        start_time = start / sr
        end_time = end / sr

        print(f"â³ è™•ç†ç¬¬ {i+1}/{total_segments} æ®µï¼Œæ™‚é–“ {start_time:.1f}â€“{end_time:.1f} ç§’")

        result = model.transcribe(
            segment_audio,
            fp16=False,      # CPU æ¨¡å¼å›ºå®šé—œé–‰ fp16
            language="zh",   # å¦‚æœæ˜¯ä¸­æ–‡å¯å›ºå®šç‚º zh
            verbose=False
        )
        text = (result.get("text") or "").strip()

        print(f"ğŸ“£ ç¬¬ {i+1} æ®µå…§å®¹ï¼š{text}\n")

        if text:
            f.write(f"[{start_time:6.1f}â€“{end_time:6.1f}s] {text}\n")

print("ğŸ‰ å…¨éƒ¨è™•ç†å®Œæˆ")
print(f"ğŸ“„ è¼¸å‡ºæ–‡å­—æª”ï¼š{out_path}")
```

## â–¶ï¸ å¦‚ä½•ä½¿ç”¨

å°‡ä½ çš„éŸ³æª”å‘½åç‚ºï¼š

```
files.wav
```

ç„¶å¾ŒåŸ·è¡Œï¼š

```bash
python transcribe_segments.py
```

ï¼ˆæˆ–ä»¥ä½ è‡ªè¨‚çš„æª”åç‚ºä¸»ï¼‰

æœƒç”¢ç”Ÿä¸€å€‹åŒå `.txt` æª”ï¼š

```
files.txt
```

è¼¸å‡ºæ ¼å¼ç¤ºä¾‹ï¼š

```
[   0.0â€“  30.0s] å—¨ï¼Œé€™æ˜¯éŸ³æª”çš„é–‹é ­éƒ¨åˆ†...
[  30.0â€“  60.0s] æ¥ä¸‹ä¾†è¬›è€…ç¹¼çºŒèªªè©±...
```


## ğŸ§  æ³¨æ„äº‹é …

* Whisper æœƒè‡ªå‹•å°‡éŸ³è¨Šè½‰ç‚º **16 kHz å–®è²é“**ã€‚
* å¤§å‹æ¨¡å‹ï¼ˆå¦‚ `medium`, `large`ï¼‰æº–ç¢ºåº¦è¼ƒé«˜ä½†éœ€è¦æ›´å¤šè¨˜æ†¶é«”ã€‚
* æƒ³ä½¿ç”¨ GPU åŠ é€Ÿï¼Œå¯å°‡ï¼š

  ```python
  fp16=True
  ```

  ä¸¦ç¢ºä¿å·²å®‰è£ CUDAã€‚
* å°¾æ®µéçŸ­çš„ç¢ç‰‡æœƒè¢«è·³éï¼Œé¿å…ç”¢ç”Ÿå¤§é‡ç©ºç™½è¼¸å‡ºã€‚


## ğŸ“œ æˆæ¬Š

æ¡ç”¨ MIT Licenseï¼Œå¯è‡ªç”±ä¿®æ”¹èˆ‡ä½¿ç”¨ã€‚


## â­ è‡´è¬

æœ¬å°ˆæ¡ˆåŸºæ–¼ **OpenAI Whisper**
[https://github.com/openai/whisper](https://github.com/openai/whisper)


